{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\"\"\"\n",
    "PPO 8 added working backtesting\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import math\n",
    "\n",
    "from data.function.load_data import load_data\n",
    "from technical_analysys.add_indicators import add_indicators\n",
    "from data.edit import normalize_data, standardize_data\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "\n",
    "class BacktestShort:\n",
    "    def __init__(self, df, look_back=20, variables=None, current_positions=True, tradable_markets='EURUSD',\n",
    "                 provision=0.0001, agent=None, initial_balance=10000, environment=None):\n",
    "        # Removed the call to the superclass constructor as it seems unnecessary\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.df_original = df.copy()\n",
    "        self.look_back = look_back\n",
    "        self.initial_balance = initial_balance\n",
    "        self.current_position = 0\n",
    "        self.variables = variables\n",
    "        self.current_positions = current_positions\n",
    "        self.tradable_markets = tradable_markets\n",
    "        self.provision = provision\n",
    "        self.agent = agent\n",
    "        self.environment = environment\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        if self.current_positions:\n",
    "            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.look_back + 1,), dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.look_back,), dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def backtest_short(self):\n",
    "        self.df[('Capital_in', self.tradable_markets)] = 0.0\n",
    "        self.df[('Capital', 'Strategy')] = 0.0\n",
    "        self.df[('PnL', self.tradable_markets)] = 0.0\n",
    "        self.df[('Provision', self.tradable_markets)] = 0.0\n",
    "        self.df.loc[self.df.index[self.look_back - 1], ('Capital_in', self.tradable_markets)] = self.initial_balance\n",
    "        self.df.loc[self.df.index[self.look_back - 1], ('Capital', 'Strategy')] = self.initial_balance\n",
    "        self.df.loc[self.df.index[self.look_back - 1], ('PnL', self.tradable_markets)] = 0.0\n",
    "        self.df.loc[self.df.index[self.look_back - 1], ('Provision', self.tradable_markets)] = 0.0\n",
    "        self.df.loc[self.df.index[self.look_back - 1], ('Action', self.tradable_markets)] = 0\n",
    "\n",
    "        for obs in range(len(self.df) - self.look_back):\n",
    "            observation = self.environment.reset(obs)\n",
    "            action = self.agent.choose_best_action(observation)\n",
    "            self.df.loc[self.df.index[obs + self.look_back], ('Action', self.tradable_markets)] = action - 1\n",
    "            self.df.loc[self.df.index[obs + self.look_back], ('PnL', self.tradable_markets)] = self.df.loc[\n",
    "                                                                                                   self.df.index[\n",
    "                                                                                                       obs + self.look_back - 1], (\n",
    "                                                                                                   'Action',\n",
    "                                                                                                   self.tradable_markets)] * \\\n",
    "                                                                                               self.df.loc[\n",
    "                                                                                                   self.df.index[\n",
    "                                                                                                       obs + self.look_back - 1], (\n",
    "                                                                                                   'Capital_in',\n",
    "                                                                                                   self.tradable_markets)] * (\n",
    "                                                                                                           self.df.loc[\n",
    "                                                                                                               self.df.index[\n",
    "                                                                                                                   obs + self.look_back], (\n",
    "                                                                                                               'Close',\n",
    "                                                                                                               self.tradable_markets)] /\n",
    "                                                                                                           self.df.loc[\n",
    "                                                                                                               self.df.index[\n",
    "                                                                                                                   obs + self.look_back - 1], (\n",
    "                                                                                                               'Close',\n",
    "                                                                                                               self.tradable_markets)] - 1) - \\\n",
    "                                                                                               self.df.loc[\n",
    "                                                                                                   self.df.index[\n",
    "                                                                                                       obs + self.look_back], (\n",
    "                                                                                                   'Provision',\n",
    "                                                                                                   self.tradable_markets)]\n",
    "            self.df.loc[self.df.index[obs + self.look_back], ('Capital', 'Strategy')] = self.df.loc[self.df.index[\n",
    "                obs + self.look_back - 1], ('Capital', 'Strategy')] + self.df.loc[self.df.index[obs + self.look_back], (\n",
    "            'PnL', self.tradable_markets)]\n",
    "\n",
    "            if self.df.loc[self.df.index[obs + self.look_back], ('Action', self.tradable_markets)] == self.df.loc[\n",
    "                self.df.index[obs + self.look_back - 1], ('Action', self.tradable_markets)]:\n",
    "                self.df.loc[self.df.index[obs + self.look_back], ('Capital_in', self.tradable_markets)] = self.df.loc[\n",
    "                    self.df.index[obs + self.look_back - 1], ('Capital_in', self.tradable_markets)]\n",
    "\n",
    "            else:\n",
    "                if self.df.loc[self.df.index[obs + self.look_back], ('Action', self.tradable_markets)] == 0:\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Capital_in', self.tradable_markets)] = 0\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Provision', self.tradable_markets)] = \\\n",
    "                    self.df.loc[\n",
    "                        self.df.index[obs + self.look_back - 1], ('Capital_in', self.tradable_markets)] * provision\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Capital', 'Strategy')] = self.df.loc[\n",
    "                                                                                                    self.df.index[\n",
    "                                                                                                        obs + self.look_back], (\n",
    "                                                                                                    'Capital',\n",
    "                                                                                                    'Strategy')] - \\\n",
    "                                                                                                self.df.loc[\n",
    "                                                                                                    self.df.index[\n",
    "                                                                                                        obs + self.look_back], (\n",
    "                                                                                                    'Provision',\n",
    "                                                                                                    self.tradable_markets)]\n",
    "\n",
    "                else:\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Capital_in', self.tradable_markets)] = \\\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Capital', 'Strategy')]\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Provision', self.tradable_markets)] = \\\n",
    "                    self.df.loc[self.df.index[obs + self.look_back - 1], (\n",
    "                    'Capital_in', self.tradable_markets)] * provision * abs(\n",
    "                        self.df.loc[self.df.index[obs + self.look_back], ('Action', self.tradable_markets)] -\n",
    "                        self.df.loc[self.df.index[obs + self.look_back - 1], ('Action', self.tradable_markets)])\n",
    "                    self.df.loc[self.df.index[obs + self.look_back], ('Capital', 'Strategy')] = self.df.loc[\n",
    "                                                                                                    self.df.index[\n",
    "                                                                                                        obs + self.look_back], (\n",
    "                                                                                                    'Capital',\n",
    "                                                                                                    'Strategy')] - \\\n",
    "                                                                                                self.df.loc[\n",
    "                                                                                                    self.df.index[\n",
    "                                                                                                        obs + self.look_back], (\n",
    "                                                                                                    'Provision',\n",
    "                                                                                                    self.tradable_markets)]\n",
    "        return self.df\n",
    "\n",
    "    def calculate_trade_outcomes(self, actions, pnl):\n",
    "        trade_outcomes = []\n",
    "        current_trade_pnl = 0\n",
    "        previous_action = None\n",
    "\n",
    "        for action, pnl_value in zip(actions, pnl):\n",
    "            if action != previous_action and action != 0:\n",
    "                if previous_action is not None:  # End of a trade\n",
    "                    trade_outcomes.append(current_trade_pnl)\n",
    "                    current_trade_pnl = 0\n",
    "            current_trade_pnl += pnl_value\n",
    "            previous_action = action\n",
    "\n",
    "        # Add the last trade if it's still open\n",
    "        if current_trade_pnl != 0:\n",
    "            trade_outcomes.append(current_trade_pnl)\n",
    "\n",
    "        return trade_outcomes\n",
    "\n",
    "    def report_short(self):\n",
    "        report = {}\n",
    "        df_log = self.df\n",
    "        trade_outcomes = self.calculate_trade_outcomes(df_log[('Action', self.tradable_markets)],\n",
    "                                                       df_log[('PnL', self.tradable_markets)])\n",
    "        profitable_trades = len([pnl for pnl in trade_outcomes if pnl > 0])\n",
    "        losing_trades = len([pnl for pnl in trade_outcomes if pnl < 0])\n",
    "        report['Win Rate (%)'] = (profitable_trades / (losing_trades + profitable_trades)) * 100 if (losing_trades + profitable_trades) != 0 else 0\n",
    "        report['Total PnL'] = self.df[('PnL', self.tradable_markets)].sum()\n",
    "        report['Sharpe Ratio'] = self.df[('PnL', self.tradable_markets)].mean() / self.df[('PnL', self.tradable_markets)].std() if self.df[('PnL', self.tradable_markets)].std() != 0 else 0\n",
    "        report['Total Return'] = self.df[('Capital', 'Strategy')].iloc[-1] / self.initial_balance - 1\n",
    "        return report\n",
    "\n",
    "    def plot(self):\n",
    "        pass\n",
    "\n",
    "    def reset(self):\n",
    "        self.df = self.df_original.copy()\n",
    "        self.current_position = 0\n",
    "\n",
    "class PPOMemory:\n",
    "    def __init__(self, batch_size):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.vals = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def generate_batches(self):\n",
    "        n_states = len(self.states)\n",
    "        batch_start = np.arange(0, n_states, self.batch_size)\n",
    "        indices = np.arange(n_states, dtype=np.int64)\n",
    "        np.random.shuffle(indices)\n",
    "        batches = [indices[i:i + self.batch_size] for i in batch_start]\n",
    "\n",
    "        return np.array(self.states), \\\n",
    "            np.array(self.actions), \\\n",
    "            np.array(self.probs), \\\n",
    "            np.array(self.vals), \\\n",
    "            np.array(self.rewards), \\\n",
    "            np.array(self.dones), \\\n",
    "            batches\n",
    "\n",
    "    def store_memory(self, state, action, probs, vals, reward, done):\n",
    "        self.states.append(state)\n",
    "        self.actions.append(action)\n",
    "        self.probs.append(probs)\n",
    "        self.vals.append(vals)\n",
    "        self.rewards.append(float(reward))\n",
    "        self.dones.append(done)\n",
    "\n",
    "    def clear_memory(self):\n",
    "        self.states = []\n",
    "        self.probs = []\n",
    "        self.actions = []\n",
    "        self.rewards = []\n",
    "        self.dones = []\n",
    "        self.vals = []\n",
    "\n",
    "\n",
    "\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, n_actions, input_dims, dropout_rate=0.25):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 1024)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc5 = nn.Linear(128, n_actions)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.fc1(state))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        x = self.softmax(self.fc5(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, input_dims, dropout_rate=0.25):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dims, 1024)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.dropout4 = nn.Dropout(dropout_rate)\n",
    "        self.fc5 = nn.Linear(128, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.fc1(state))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.relu(self.fc3(x))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.relu(self.fc4(x))\n",
    "        x = self.dropout4(x)\n",
    "        q = self.fc5(x)\n",
    "        return q\n",
    "\n",
    "\n",
    "class PPO_Agent:\n",
    "    def __init__(self, n_actions, input_dims, gamma=0.95, alpha=0.001, gae_lambda=0.9, policy_clip=0.2, batch_size=1024, n_epochs=20, mini_batch_size=128):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # TODO repair cuda\n",
    "        # self.device = torch.device(\"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        self.gamma = gamma\n",
    "        self.policy_clip = policy_clip\n",
    "        self.n_epochs = n_epochs\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.actor = ActorNetwork(n_actions, input_dims).to(self.device)\n",
    "        self.critic = CriticNetwork(input_dims).to(self.device)\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=alpha)\n",
    "        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=alpha)\n",
    "        self.memory = PPOMemory(batch_size)\n",
    "\n",
    "    def store_transition(self, state, action, probs, vals, reward, done):\n",
    "        self.memory.store_memory(state, action, probs, vals, reward, done)\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.n_epochs):\n",
    "            # Generating the data for the entire batch\n",
    "            state_arr, action_arr, old_prob_arr, vals_arr, reward_arr, dones_arr, _ = self.memory.generate_batches()\n",
    "\n",
    "            values = torch.tensor(vals_arr, dtype=torch.float).to(self.device)\n",
    "            advantage = np.zeros(len(reward_arr), dtype=np.float32)\n",
    "\n",
    "            # Calculating Advantage\n",
    "            for t in range(len(reward_arr) - 1):\n",
    "                discount = 1\n",
    "                a_t = 0\n",
    "                for k in range(t, len(reward_arr) - 1):\n",
    "                    a_t += discount * (reward_arr[k] + self.gamma * values[k + 1] * (1 - int(dones_arr[k])) - values[k])\n",
    "                    discount *= self.gamma * self.gae_lambda\n",
    "                advantage[t] = a_t\n",
    "\n",
    "            advantage = torch.tensor(advantage, dtype=torch.float).to(self.device)\n",
    "\n",
    "            # Creating mini-batches\n",
    "            num_samples = len(state_arr)\n",
    "            indices = np.arange(num_samples)\n",
    "            np.random.shuffle(indices)\n",
    "            for start_idx in range(0, num_samples, self.mini_batch_size):\n",
    "                # Extract indices for the mini-batch\n",
    "                minibatch_indices = indices[start_idx:start_idx + self.mini_batch_size]\n",
    "\n",
    "                # Extract data for the current mini-batch\n",
    "                batch_states = torch.tensor(state_arr[minibatch_indices], dtype=torch.float).to(self.device)\n",
    "                batch_actions = torch.tensor(action_arr[minibatch_indices], dtype=torch.long).to(self.device)\n",
    "                batch_old_probs = torch.tensor(old_prob_arr[minibatch_indices], dtype=torch.float).to(self.device)\n",
    "                batch_advantages = advantage[minibatch_indices]\n",
    "                batch_values = values[minibatch_indices]\n",
    "\n",
    "                self.actor_optimizer.zero_grad()\n",
    "                self.critic_optimizer.zero_grad()\n",
    "\n",
    "                # Actor Network Loss\n",
    "                probs = self.actor(batch_states)\n",
    "                dist = torch.distributions.Categorical(probs)\n",
    "                new_probs = dist.log_prob(batch_actions)\n",
    "                prob_ratio = torch.exp(new_probs - batch_old_probs)\n",
    "                weighted_probs = batch_advantages * prob_ratio\n",
    "                clipped_probs = torch.clamp(prob_ratio, 1 - self.policy_clip, 1 + self.policy_clip)\n",
    "                weighted_clipped_probs = clipped_probs * batch_advantages\n",
    "                actor_loss = -torch.min(weighted_probs, weighted_clipped_probs).mean()\n",
    "\n",
    "                # Critic Network Loss\n",
    "                critic_value = self.critic(batch_states).squeeze()\n",
    "                returns = batch_advantages + batch_values\n",
    "                critic_loss = nn.functional.mse_loss(critic_value, returns)\n",
    "\n",
    "                # Gradient Calculation and Optimization Step\n",
    "                actor_loss.backward()\n",
    "                critic_loss.backward()\n",
    "                self.actor_optimizer.step()\n",
    "                self.critic_optimizer.step()\n",
    "\n",
    "        self.memory.clear_memory()\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        \"\"\"\n",
    "        Selects an action based on the current policy and exploration noise.\n",
    "\n",
    "        \"\"\"\n",
    "        if not isinstance(observation, np.ndarray):\n",
    "            observation = np.array(observation)\n",
    "\n",
    "        observation = np.array(observation).reshape(1, -1)\n",
    "        state = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
    "        probs = self.actor(state)\n",
    "\n",
    "        dist = torch.distributions.Categorical(probs)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "        value = self.critic(state)\n",
    "\n",
    "        return action.item(), log_prob.item(), value.item()\n",
    "\n",
    "    def get_action_probabilities(self, observation):\n",
    "        \"\"\"\n",
    "        Returns the probabilities of each action for a given observation.\n",
    "        \"\"\"\n",
    "        observation = np.array(observation).reshape(1, -1)\n",
    "        state = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(state)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    def choose_best_action(self, observation):\n",
    "        \"\"\"\n",
    "        Selects the best action based on the current policy without exploration.\n",
    "        \"\"\"\n",
    "        if not isinstance(observation, np.ndarray):\n",
    "            observation = np.array(observation)\n",
    "\n",
    "        observation = np.array(observation).reshape(1, -1)\n",
    "        state = torch.tensor(observation, dtype=torch.float).to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = self.actor(state)\n",
    "\n",
    "        best_action = torch.argmax(probs, dim=1).item()\n",
    "        return best_action\n",
    "\n",
    "\n",
    "class Trading_Environment_Basic(gym.Env):\n",
    "    def __init__(self, df, look_back=20, variables=None, current_positions=True, tradble_markets='EURUSD', provision=0.0001):\n",
    "        super(Trading_Environment_Basic, self).__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.look_back = look_back\n",
    "        self.initial_balance = 10000\n",
    "        self.current_position = 0\n",
    "        self.variables = variables\n",
    "        self.current_positions = current_positions\n",
    "        self.tradable_markets = tradble_markets\n",
    "        self.provision = provision\n",
    "\n",
    "        # Define action and observation space\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "        if self.current_positions:\n",
    "            self.observation_space = spaces.Box(low=-np.inf,\n",
    "                                                high=np.inf,\n",
    "                                                shape=(look_back + 1,),  # +1 for current position\n",
    "                                                dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = spaces.Box(low=-np.inf,\n",
    "                                                high=np.inf,\n",
    "                                                shape=(look_back,),\n",
    "                                                dtype=np.float32)\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def calculate_input_dims(self):\n",
    "        num_variables = len(self.variables)  # Number of variables\n",
    "        input_dims = num_variables * self.look_back  # Variables times look_back\n",
    "        if self.current_positions:\n",
    "            input_dims += 1  # Add one more dimension for current position\n",
    "        return input_dims\n",
    "\n",
    "    def reset(self, day=None):\n",
    "        if day is not None:\n",
    "            self.current_step = day + self.look_back\n",
    "        else:\n",
    "            self.current_step = self.look_back\n",
    "\n",
    "        self.balance = self.initial_balance\n",
    "        self.done = False\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _create_base_observation(self):\n",
    "        start = max(self.current_step - self.look_back, 0)\n",
    "        end = self.current_step\n",
    "        return self.df[self.variables].iloc[start:end].values\n",
    "\n",
    "    def _next_observation(self):\n",
    "        start = max(self.current_step - self.look_back, 0)\n",
    "        end = self.current_step\n",
    "\n",
    "        # Apply scaling based on 'edit' property of each variable\n",
    "        scaled_observation = []\n",
    "        for variable in self.variables:\n",
    "            data = self.df[variable['variable']].iloc[start:end].values\n",
    "            if variable['edit'] == 'standardize':\n",
    "                scaled_data = standardize_data(data)\n",
    "            elif variable['edit'] == 'normalize':\n",
    "                scaled_data = normalize_data(data)\n",
    "            else:  # Default to normalization\n",
    "                scaled_data = data\n",
    "\n",
    "            scaled_observation.extend(scaled_data)\n",
    "\n",
    "        if self.current_positions:\n",
    "            scaled_observation = np.append(scaled_observation, (self.current_position+1)/2)\n",
    "\n",
    "        return np.array(scaled_observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        # Existing action mapping\n",
    "        action_mapping = {0: -1, 1: 0, 2: 1}\n",
    "        mapped_action = action_mapping[action]\n",
    "\n",
    "        # Get current and next price\n",
    "        current_price = self.df[('Close', self.tradable_markets)].iloc[self.current_step]\n",
    "        next_price = self.df[('Close', self.tradable_markets)].iloc[self.current_step + 1]\n",
    "\n",
    "        # Calculate log return based on action\n",
    "        log_return = math.log(next_price / current_price) if current_price != 0 else 0\n",
    "        reward = 0\n",
    "        if mapped_action == 1:  # Buying\n",
    "            reward = log_return\n",
    "        elif mapped_action == -1:  # Selling\n",
    "            reward = -log_return\n",
    "\n",
    "        # Calculate cost based on action and current position\n",
    "        if mapped_action != self.current_position:\n",
    "            if abs(mapped_action - self.current_position) == 2:\n",
    "                provision = math.log(1 - 2 * self.provision)\n",
    "            else:\n",
    "                provision = math.log(1 - self.provision)\n",
    "        else:\n",
    "            provision = 0\n",
    "        reward += provision\n",
    "\n",
    "        # Update the balance\n",
    "        self.balance *= math.exp(reward)  # Update balance using exponential of reward\n",
    "\n",
    "        # Update current position and step\n",
    "        self.current_position = mapped_action\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Check if the episode is done\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            self.done = True\n",
    "\n",
    "        return self._next_observation(), reward, self.done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        if mode == 'human':\n",
    "            window_start = max(0, self.current_step - self.look_back)\n",
    "            window_end = self.current_step + 1\n",
    "            plt.figure(figsize=(10, 6))\n",
    "\n",
    "            # Plotting the price data\n",
    "            plt.plot(self.df[('Close', self.tradable_markets)].iloc[window_start:window_end], label='Close Price')\n",
    "\n",
    "            # Highlighting the current position: Buy (1), Sell (-1), Hold (0)\n",
    "            if self.current_position == 1:\n",
    "                plt.scatter(self.current_step, self.df[('Close', self.tradable_markets)].iloc[self.current_step],\n",
    "                            color='green', label='Buy')\n",
    "            elif self.current_position == -1:\n",
    "                plt.scatter(self.current_step, self.df[('Close', self.tradable_markets)].iloc[self.current_step],\n",
    "                            color='red', label='Sell')\n",
    "\n",
    "            plt.title('Trading Environment State')\n",
    "            plt.xlabel('Time Step')\n",
    "            plt.ylabel('Price')\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "df = load_data(['EURUSD', 'USDJPY', 'EURJPY'], '1D')\n",
    "\n",
    "indicators = [\n",
    "    {\"indicator\": \"RSI\", \"mkf\": \"EURUSD\", \"length\": 14},\n",
    "    {\"indicator\": \"RSI\", \"mkf\": \"EURJPY\", \"length\": 14},\n",
    "    {\"indicator\": \"RSI\", \"mkf\": \"USDJPY\", \"length\": 14},\n",
    "    {\"indicator\": \"ATR\", \"mkf\": \"EURUSD\", \"length\": 24},\n",
    "    {\"indicator\": \"ATR\", \"mkf\": \"EURJPY\", \"length\": 24},\n",
    "    {\"indicator\": \"SMA\", \"mkf\": \"EURUSD\", \"length\": 100},\n",
    "    {\"indicator\": \"ATR\", \"mkf\": \"USDJPY\", \"length\": 24},\n",
    "    {\"indicator\": \"MACD\", \"mkf\": \"EURUSD\"},\n",
    "    {\"indicator\": \"Stochastic\", \"mkf\": \"USDJPY\"},\n",
    "\n",
    "]\n",
    "add_indicators(df, indicators)\n",
    "df = df.dropna()\n",
    "start_date = '2014-01-01'\n",
    "validation_date = '2021-01-01'\n",
    "test_date = '2022-01-01'\n",
    "df_train = df[start_date:validation_date]\n",
    "df_validation = df[validation_date:test_date]\n",
    "df_test = df[test_date:]\n",
    "\n",
    "variables = [\n",
    "    {\"variable\": (\"RSI_14\", \"EURUSD\"), \"edit\": \"normalize\"},\n",
    "    {\"variable\": (\"ATR_24\", \"EURUSD\"), \"edit\": \"normalize\"},\n",
    "    {\"variable\": (\"Close\", \"USDJPY\"), \"edit\": \"normalize\"},\n",
    "    {\"variable\": (\"Close\", \"EURUSD\"), \"edit\": \"normalize\"},\n",
    "    {\"variable\": (\"Close\", \"EURJPY\"), \"edit\": \"normalize\"}\n",
    "]\n",
    "tradable_markets = 'EURUSD'\n",
    "\n",
    "print('number of train samples: ', len(df_train))\n",
    "print('number of validation samples: ', len(df_validation))\n",
    "print('number of test samples: ', len(df_test))\n",
    "\n",
    "look_back = 20\n",
    "provision = 0.001  # 0.001, cant be too high as it would not learn to trade\n",
    "batch_size = 2048\n",
    "epochs = 30  # 40\n",
    "mini_batch_size = 256\n",
    "# Create the environment\n",
    "env = Trading_Environment_Basic(df_train, look_back=look_back, variables=variables, current_positions=True, tradble_markets=tradable_markets, provision=provision)\n",
    "agent = PPO_Agent(n_actions=env.action_space.n, input_dims=env.calculate_input_dims(), batch_size=batch_size, n_epochs=epochs, mini_batch_size=mini_batch_size)\n",
    "\n",
    "num_episodes = 60  # 100\n",
    "\n",
    "total_rewards = []\n",
    "episode_durations = []\n",
    "total_balances = []\n",
    "index = pd.MultiIndex.from_product([range(num_episodes), ['validation', 'test']], names=['episode', 'dataset'])\n",
    "columns = ['Win Rate (%)', 'Total PnL', 'Sharpe Ratio', 'Total Return']\n",
    "backtest_results = pd.DataFrame(index=index, columns=columns)\n",
    "\n",
    "for episode in tqdm(range(num_episodes)):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    cumulative_reward = 0\n",
    "    start_time = time.time()\n",
    "    initial_balance = env.balance\n",
    "    while not done:\n",
    "        action, prob, val = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.store_transition(observation, action, prob, val, reward, done)\n",
    "        observation = observation_\n",
    "        cumulative_reward += reward\n",
    "        if len(agent.memory.states) == agent.memory.batch_size:\n",
    "            agent.learn()\n",
    "\n",
    "    end_time = time.time()\n",
    "    episode_time = end_time - start_time\n",
    "    total_rewards.append(cumulative_reward)\n",
    "    episode_durations.append(episode_time)\n",
    "    total_balances.append(env.balance)\n",
    "    calculated_final_balance = initial_balance * math.exp(cumulative_reward)\n",
    "\n",
    "    if episode % 1 == 0:\n",
    "        print(f'\\nEpisode: {episode + 1}')\n",
    "        print(f\"Episode {episode + 1}: Total Reward: {cumulative_reward}, Total Balance: {env.balance:.2f}, Duration: {episode_time:.2f} seconds\")\n",
    "        print('----\\n')\n",
    "        # Backtesting on validation dataset\n",
    "        validation_backtester = BacktestShort(df=df_validation, look_back=look_back, variables=variables, current_positions=True, tradable_markets=tradable_markets, provision=provision, agent=agent, initial_balance=10000,environment=env)\n",
    "        validation_backtester.backtest_short()\n",
    "        validation_report = validation_backtester.report_short()\n",
    "\n",
    "        # Backtesting on test dataset\n",
    "        test_backtester = BacktestShort(df=df_test, look_back=look_back, variables=variables, current_positions=True, tradable_markets=tradable_markets, provision=provision, agent=agent, initial_balance=10000, environment=env)\n",
    "        test_backtester.backtest_short()\n",
    "        test_report = test_backtester.report_short()\n",
    "\n",
    "        # Store backtesting results\n",
    "        backtest_results.loc[(episode, 'validation')] = validation_report\n",
    "        backtest_results.loc[(episode, 'test')] = test_report\n",
    "\n",
    "print(backtest_results)\n",
    "\n",
    "# Plotting the results after all episodes\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting the results after all episodes\n",
    "plt.plot(total_rewards)\n",
    "plt.title('Total Reward Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the results after all episodes\n",
    "plt.plot(episode_durations, color='red')\n",
    "plt.title('Episode Duration Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "# Plotting the results after all episodes\n",
    "plt.plot(total_balances, color='green')\n",
    "plt.title('Total Balance Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.show()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "'Win Rate (%)', 'Total PnL', 'Sharpe Ratio', 'Total Return'\n",
    "# Extracting data for plotting\n",
    "validation_pnl = backtest_results.loc[(slice(None), 'validation'), 'Total PnL']\n",
    "test_pnl = backtest_results.loc[(slice(None), 'test'), 'Total PnL']\n",
    "\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), validation_pnl.values, label='Validation Total PnL', marker='o')\n",
    "plt.plot(range(num_episodes), test_pnl.values, label='Test Total PnL', marker='x')\n",
    "\n",
    "plt.title('Total PnL Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total PnL')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation_Win = backtest_results.loc[(slice(None), 'validation'), 'Win Rate (%)']\n",
    "test_Win = backtest_results.loc[(slice(None), 'test'), 'Win Rate (%)']\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), validation_Win.values, label='Validation Win Rate (%)', marker='o')\n",
    "plt.plot(range(num_episodes), test_Win.values, label='Test Win Rate (%)', marker='x')\n",
    "\n",
    "plt.title('Total Win Rate (%) Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win Rate (%)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "validation_Sharpe = backtest_results.loc[(slice(None), 'validation'), 'Sharpe Ratio']\n",
    "test_Sharpe = backtest_results.loc[(slice(None), 'test'), 'Sharpe Ratio']\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), validation_Sharpe.values, label='Validation Sharpe Ratio', marker='o')\n",
    "plt.plot(range(num_episodes), test_Sharpe.values, label='Test Sharpe Ratio', marker='x')\n",
    "\n",
    "plt.title('Sharpe Ratio Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Sharpe Ratio')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "validation_Return = backtest_results.loc[(slice(None), 'validation'), 'Total Return']\n",
    "test_Return = backtest_results.loc[(slice(None), 'test'), 'Total Return']\n",
    "# Creating the plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), validation_Return.values, label='Validation Total Return', marker='o')\n",
    "plt.plot(range(num_episodes), test_Return.values, label='Test Total Return', marker='x')\n",
    "\n",
    "plt.title('Total Return Over Episodes')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Return')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# final prediction agent\n",
    "# TEST\n",
    "df_test_probs = df_test.copy()\n",
    "predictions_df = pd.DataFrame(index=df_test.index, columns=['Predicted_Action'])\n",
    "test_env = Trading_Environment_Basic(df_test, look_back=look_back, variables=variables, current_positions=True, tradble_markets=tradable_markets)\n",
    "\n",
    "for test_day in range(len(df_test) - test_env.look_back):\n",
    "    observation = test_env.reset(test_day)\n",
    "    action = agent.choose_best_action(observation)\n",
    "    predictions_df.iloc[test_day + test_env.look_back] = action\n",
    "\n",
    "# Merge with df_test\n",
    "df_test_with_predictions = df_test.copy()\n",
    "df_test_with_predictions['Predicted_Action'] = predictions_df['Predicted_Action'] - 1\n",
    "\n",
    "\n",
    "# final prediction with probabilities\n",
    "test_env_probs = Trading_Environment_Basic(df_test_probs, look_back=look_back, variables=variables, current_positions=True, tradble_markets=tradable_markets)\n",
    "action_probabilities = []\n",
    "\n",
    "for test_day in range(len(df_test_probs) - test_env_probs.look_back):\n",
    "    observation = test_env_probs.reset(test_day)  # Reset environment to the specific day\n",
    "    probs = agent.get_action_probabilities(observation)\n",
    "    action_probabilities.append(probs[0])\n",
    "\n",
    "# Convert the list of probabilities to a DataFrame\n",
    "probabilities_df = pd.DataFrame(action_probabilities, columns=['Short', 'Do_nothing', 'Long'])\n",
    "\n",
    "# Join with the original test DataFrame\n",
    "df_test_with_probabilities = df_test_probs.iloc[test_env_probs.look_back:].reset_index(drop=True)\n",
    "df_test_with_probabilities = pd.concat([df_test_with_probabilities, probabilities_df], axis=1)\n",
    "\n",
    "# TRAIN\n",
    "df_train_probs = df_train.copy()\n",
    "predictions_df = pd.DataFrame(index=df_train.index, columns=['Predicted_Action'])\n",
    "train_env = Trading_Environment_Basic(df_train, look_back=look_back, variables=variables, current_positions=True, tradble_markets=tradable_markets)\n",
    "\n",
    "for train_day in range(len(df_train) - train_env.look_back):\n",
    "    observation = train_env.reset(train_day)\n",
    "    action = agent.choose_best_action(observation)\n",
    "    predictions_df.iloc[train_day + train_env.look_back] = action\n",
    "\n",
    "# Merge with df_train\n",
    "df_train_with_predictions = df_train.copy()\n",
    "df_train_with_predictions['Predicted_Action'] = predictions_df['Predicted_Action'] - 1\n",
    "\n",
    "\n",
    "train_env_probs = Trading_Environment_Basic(df_train_probs, look_back=look_back, variables=variables, current_positions=True, tradble_markets=tradable_markets)\n",
    "action_probabilities = []\n",
    "\n",
    "for train_day in range(len(df_train_probs) - train_env_probs.look_back):\n",
    "    observation = train_env_probs.reset(train_day)  # Reset environment to the specific day\n",
    "    probs = agent.get_action_probabilities(observation)\n",
    "    action_probabilities.append(probs[0])\n",
    "\n",
    "# Convert the list of probabilities to a DataFrame\n",
    "probabilities_df = pd.DataFrame(action_probabilities, columns=['Short', 'Do_nothing', 'Long'])\n",
    "\n",
    "# Join with the original train DataFrame\n",
    "df_train_with_probabilities = df_train_probs.iloc[train_env_probs.look_back:].reset_index(drop=True)\n",
    "df_train_with_probabilities = pd.concat([df_train_with_probabilities, probabilities_df], axis=1)\n",
    "\n",
    "''' \n",
    "def report(self):\n",
    "    report = {}\n",
    "    df_log = self.df\n",
    "    trade_outcomes = self.calculate_trade_outcomes(df_log['Action'], df_log['PnL'])\n",
    "\n",
    "    total_trades = self.count_trades(df_log['Action'])\n",
    "    # Profitable and losing trades\n",
    "    profitable_trades = len([pnl for pnl in trade_outcomes if pnl > 0])\n",
    "    losing_trades = len([pnl for pnl in trade_outcomes if pnl < 0])\n",
    "\n",
    "    # Win rate\n",
    "    win_rate = (profitable_trades / total_trades) * 100 if total_trades > 0 else 0\n",
    "\n",
    "    # Adding metrics to the report\n",
    "    report['Total Trades'] = total_trades\n",
    "    report['Profitable Trades'] = profitable_trades\n",
    "    report['Losing Trades'] = losing_trades\n",
    "    report['Win Rate (%)'] = win_rate\n",
    "\n",
    "    report['Total PnL'] = self.df[('PnL', self.tradable_markets)].sum().sum()\n",
    "    report['Total Return'] = self.df[('Capital', 'Strategy')].iloc[-1] / self.df[('Capital', 'Strategy')].iloc[0] - 1\n",
    "    report['Total Provisions'] = self.df[('Provisions'].sum().sum()\n",
    "    report['Sortino Ratio'] = self.calculate_sortino_ratio(self.df)\n",
    "    report['Sharpe Ratio'] = total_pnl.mean() / total_pnl.std() if total_pnl.std() != 0 else 0\n",
    "    report['Max Drawdown'] = self.calculate_max_drawdown(self.df)\n",
    "    report['Max Drawdown Duration'] = self.df[('Capital', 'Strategy')].idxmin() - self.df[('Capital', 'Strategy')].idxmax()\n",
    "    report['Total Trades'] = total_trades\n",
    "    report['Winning Trades'] = winning_trades\n",
    "    report['Losing Trades'] = losing_trades\n",
    "    report['Win Rate (%)'] = (winning_trades / total_trades) * 100\n",
    "    report['Average Gain ($)'] = df_log['PnL'].mean()\n",
    "\n",
    "    return report\n",
    "'''\n"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
